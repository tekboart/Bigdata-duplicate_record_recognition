{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/03 23:50:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "\n",
    "# PySpark specific imports\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Special version of pandas for spark\n",
    "from pyspark import pandas as ps\n",
    "ps.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Defined functions (for code re-usability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_all(dfs: list):\n",
    "    '''\n",
    "    combines multiple Spark DataFrames in a given list\n",
    "\n",
    "    input: a list of Spark DataFrames\n",
    "    output: a single/unified Spark DataFrames\n",
    "    '''\n",
    "    return reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
    "\n",
    "def shape_df(a_spark_dataframe) -> tuple:\n",
    "    '''\n",
    "    input: a Spark data frame\n",
    "    output: shape of the DataFrame (as a tuple of row & col --> (#rows, #cols) )\n",
    "    '''\n",
    "    return (a_spark_dataframe.count(), len(a_spark_dataframe.columns))\n",
    "\n",
    "def cast_col_type(a_spark_df, col_list: list, new_type: str):\n",
    "    ''''\n",
    "    input: \n",
    "        1. a pyspark dataframe (to be used as reference)\n",
    "        2. a list of col names (to convert their value types)\n",
    "        3. new type (to convert values to)\n",
    "            e.g., 'integer', 'float', 'string', 'double', and etc.\n",
    "    output: a DataFrame with the specified col type changed new_type\n",
    "    '''\n",
    "    return a_spark_df.select(\n",
    "        *[c for c in a_spark_df.columns if c not in col_list], \n",
    "        *[col(feat).cast(new_type).alias(feat) for feat in col_list]\n",
    "        )\n",
    "\n",
    "def missing_imputer(df_pyspark, col_list, strategy='mean'):\n",
    "    '''\n",
    "    filling missing values by the defined \"measure of centeral tendency\"\n",
    "    input:\n",
    "        1. a spark's DataFrame\n",
    "        2. a list of cols/feats to be used for imputation\n",
    "        3. the strategy by which missign values are filled\n",
    "            e.g., mean, median, mode, etc.\n",
    "    output: the imputed spark DataFrame\n",
    "    '''\n",
    "    imputer = Imputer(\n",
    "        inputCols=col_list,\n",
    "        outputCols=col_list\n",
    "    ).setStrategy(strategy)\n",
    "    return imputer.fit(df_pyspark).transform(df_pyspark)\n",
    "\n",
    "def balance_dataset(df_pyspark, label_col='label'):\n",
    "    ''' \n",
    "    Re-balancing (weighting) of records in case of imbalanced #labels \n",
    "        * e.g., in healthcare or fraud detection the causes \n",
    "\n",
    "    fromula to calc weight for label j:\n",
    "        w_j = n_recs / (n_labels * n_recs_j)  \n",
    "\n",
    "    input: \n",
    "        1. df_pyspark: a spark DataFrame\n",
    "        2. label_col: the name the label/class col\n",
    "            * note that the labels must be integers (e.g., 0, 1, 2, ...)\n",
    "        3. important_label = the label/class which is the important one\n",
    "\n",
    "    output: a spark DataFrame with an added 'class_weight' to be used for classification\n",
    "        e.g., to be used in the logistic loss objective function\n",
    "    '''\n",
    "    n_recs = df_pyspark.count()\n",
    "    labels = sorted(list(train.select(label_col).distinct().toPandas()[label_col]))\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    labels_freq = {label: df_pyspark.filter(df_pyspark[label_col]==label).count() for label in labels}\n",
    "    labels_weights = {label: (n_recs / (n_labels * labels_freq[label])) for label in labels}\n",
    "\n",
    "    # define a udf (user defined function) to return the weight with respect to the label of each record\n",
    "    assign_weights = udf(lambda x: labels_weights[x])\n",
    "\n",
    "    return df_pyspark.withColumn(\"class_weight\", assign_weights(col('is_match')).cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pandas-on-Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1130271eb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('bigdata_final_project_preprocessing').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "- import each chunk individually\n",
    "- combine all chunks together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import all dataset chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('./Data/block_1.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('./Data/block_2.csv', header=True, inferSchema=True)\n",
    "df3 = spark.read.csv('./Data/block_3.csv', header=True, inferSchema=True)\n",
    "df4 = spark.read.csv('./Data/block_4.csv', header=True, inferSchema=True)\n",
    "df5 = spark.read.csv('./Data/block_5.csv', header=True, inferSchema=True)\n",
    "df6 = spark.read.csv('./Data/block_6.csv', header=True, inferSchema=True)\n",
    "df7 = spark.read.csv('./Data/block_7.csv', header=True, inferSchema=True)\n",
    "df8 = spark.read.csv('./Data/block_8.csv', header=True, inferSchema=True)\n",
    "df9 = spark.read.csv('./Data/block_9.csv', header=True, inferSchema=True)\n",
    "df10 = spark.read.csv('./Data/block_10.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine all chunks of dataset into one cohesive unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|37291|53113|0.833333333333333|           ?|         1.0|           ?|      1|     1|     1|     1|      0|    true|\n",
      "|39086|47614|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|70031|70237|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|84795|97439|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|36950|42116|                1|           ?|         1.0|           1|      1|     1|     1|     1|      1|    true|\n",
      "|42413|48491|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|25965|64753|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|49451|90407|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      0|    true|\n",
      "|39932|40902|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|46626|47940|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|48948|98379|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "| 4767| 4826|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|45463|69659|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|11367|13169|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|10782|89636|                1|           ?|         1.0|           ?|      1|     0|     1|     1|      1|    true|\n",
      "|26206|39147|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|16662|27083|                1|           1|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|18823|30204|                1|           1|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|38545|85418|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "|28963|39172|                1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10]\n",
    "\n",
    "# combine all chunks of dataset into one cohesive unit\n",
    "df = union_all(df_list)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Dataset\n",
    "- replace \"?\" signes with Null values\n",
    "- convert (cast) all col types with new (correct) type\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace \"?\" signes with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the dataset (before preprocessing): (5749132, 12)\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|37291|53113|0.833333333333333|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|39086|47614|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|70031|70237|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|84795|97439|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|36950|42116|                1|        null|         1.0|           1|      1|     1|     1|     1|      1|    true|\n",
      "|42413|48491|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|25965|64753|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|49451|90407|                1|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|39932|40902|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|46626|47940|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|48948|98379|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "| 4767| 4826|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|45463|69659|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|11367|13169|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|10782|89636|                1|        null|         1.0|        null|      1|     0|     1|     1|      1|    true|\n",
      "|26206|39147|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|16662|27083|                1|           1|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|18823|30204|                1|           1|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|38545|85418|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|28963|39172|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace ? with Null values\n",
    "# as ? is not known as missing value to pyspark\n",
    "df_with_null = df.na.replace('?', None)\n",
    "\n",
    "print('the shape of the dataset (before preprocessing):', shape_df(df_with_null))\n",
    "df_with_null.show()\n",
    "df_with_null.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert (cast) all col types with new (correct) type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -----------------------------Before type conversion----------------------------\n",
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n",
      "\n",
      " -----------------------------After type conversion-----------------------------\n",
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the value type of each feat/col\n",
    "# as we see the Schemas are not correct\n",
    "# e.g., we have string for cmp_fname_c1 which must be 'float'/'double'\n",
    "print('\\n','Before type conversion'.center(79, \"-\"))\n",
    "df_with_null.printSchema()\n",
    "\n",
    "# make the cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2 cols double_precision float\n",
    "df_new_type_1 = cast_col_type(df_with_null, df.columns[2:6], 'double')\n",
    "# df_new_type_1.show(21000)\n",
    "# df_new_type_1.printSchema()\n",
    "\n",
    "# make the cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match cols integer\n",
    "df_new_type_2 = cast_col_type(df_new_type_1, df.columns[6:11], 'int')\n",
    "# df_new_type_2.show(21000)\n",
    "# df_new_type_2.printSchema()\n",
    "\n",
    "# make the labels/classes binary (0, 1) instead of True/False\n",
    "df_new_type_3 = cast_col_type(df_new_type_2, ['is_match'], 'integer')\n",
    "# df_new_type_3.show(21000)\n",
    "# df_new_type_3.printSchema()\n",
    "\n",
    "df_new_type_final = df_new_type_3\n",
    "print('\\n','After type conversion'.center(79, \"-\"))\n",
    "df_new_type_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "- remove recs with more than a threshold Missing values\n",
    "- replace the Missing values with 'mean', 'median', or 'mode' of their respective col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove recs with more than a threshold Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==================================================>     (63 + 7) / 70]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed recs with more than threshold Null values: 793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "threshold = 8\n",
    "df_rm_null_by_thresh = df_new_type_final.dropna(thresh=threshold)\n",
    "print('number of removed recs with more than threshold Null values:',\n",
    "      shape_df(df_new_type_final)[0] - shape_df(df_rm_null_by_thresh)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove the cols with high #Null values or insignificant ones.\n",
    "the removed cols are:\n",
    "- cmp_lname_c2\n",
    "- cmp_fname_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_lname_c1|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+-------+------+------+------+-------+--------+\n",
      "|37291|53113|0.833333333333333|         1.0|      1|     1|     1|     1|      0|       1|\n",
      "|39086|47614|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|70031|70237|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|84795|97439|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|36950|42116|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|42413|48491|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|25965|64753|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|49451|90407|              1.0|         1.0|      1|     1|     1|     1|      0|       1|\n",
      "|39932|40902|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|46626|47940|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|48948|98379|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "| 4767| 4826|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|45463|69659|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|11367|13169|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|10782|89636|              1.0|         1.0|      1|     0|     1|     1|      1|       1|\n",
      "|26206|39147|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|16662|27083|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|18823|30204|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|38545|85418|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "|28963|39172|              1.0|         1.0|      1|     1|     1|     1|      1|       1|\n",
      "+-----+-----+-----------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_removed_insig_feats = df_rm_null_by_thresh.drop(*['cmp_fname_c2', 'cmp_lname_c2'])\n",
    "df_removed_insig_feats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into train & test\n",
    "- we split the data inot 0.9, 0.1 splits for train and test sets respectively.\n",
    "- we didn't use 0.7, 0.3 split because we are dealing with big data and as data grows the give less portion of the data to test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------the distribution of labels for train set-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|       1|  14702|\n",
      "|       0|4008522|\n",
      "+--------+-------+\n",
      "\n",
      "--------------------the distribution of labels for test set--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=====================================================>  (67 + 3) / 70]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|       1|   6223|\n",
      "|       0|1718892|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, test = df_removed_insig_feats.randomSplit([0.7, 0.3], seed = 18)\n",
    "\n",
    "print(\"the distribution of labels for train set\".center(79, \"-\"))\n",
    "train.groupBy('is_match').count().show()\n",
    "print(\"the distribution of labels for test set\".center(79, \"-\"))\n",
    "test.groupBy('is_match').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace Missing values of: \n",
    "- numerical real valued cols with the 'mean' of their respective col\n",
    "- numerical binary valued cols with the 'mode' of their respective col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. replace the Missing values for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#NaN or #Null values in each col of train set (after imputation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_1  id_2  cmp_fname_c1  cmp_lname_c1  cmp_sex  cmp_bd  cmp_bm  cmp_by  \\\n",
       "0     0     0             0             0        0       0       0       0   \n",
       "\n",
       "   cmp_plz  \n",
       "0        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "\n",
    "\n",
    "train_imputed_1 = missing_imputer(train, ['cmp_fname_c1', 'cmp_lname_c1'], 'mean')\n",
    "train_imputed_2 = missing_imputer(train_imputed_1, ['cmp_sex','cmp_bd','cmp_bm','cmp_by','cmp_plz'], 'mode')\n",
    "\n",
    "print('\\n#NaN or #Null values in each col of train set (after imputation)')\n",
    "count_miss_train = train_imputed_2.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    "     for c in train.columns if c != 'is_match']\n",
    ")\n",
    "\n",
    "display(count_miss_train.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. replace the Missing values for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#NaN or #Null values in each col of test set (after imputation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_1  id_2  cmp_fname_c1  cmp_lname_c1  cmp_sex  cmp_bd  cmp_bm  cmp_by  \\\n",
       "0     0     0             0             0        0       0       0       0   \n",
       "\n",
       "   cmp_plz  \n",
       "0        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_imputed_1 = missing_imputer(test, ['cmp_fname_c1', 'cmp_lname_c1'], 'mean')\n",
    "test_imputed_2 = missing_imputer(test_imputed_1, ['cmp_sex','cmp_bd','cmp_bm','cmp_by','cmp_plz'], 'mode')\n",
    "\n",
    "print('\\n#NaN or #Null values in each col of test set (after imputation)')\n",
    "count_miss_test = test_imputed_2.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    "     for c in test.columns if c != 'is_match']\n",
    ")\n",
    "\n",
    "display(count_miss_test.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Class_Weight, as our labels are unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------------+------------------+-------+------+------+------+-------+--------+------------------+\n",
      "|id_1| id_2|     cmp_fname_c1|      cmp_lname_c1|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|      class_weight|\n",
      "+----+-----+-----------------+------------------+-------+------+------+------+-------+--------+------------------+\n",
      "|   2|78418|              1.0|               0.5|      1|     0|     0|     1|      0|       0|0.5018338429974938|\n",
      "|   5|46062|0.222222222222222|               1.0|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|   6|45145|              1.0| 0.222222222222222|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|   6|51814|              1.0| 0.166666666666667|      0|     1|     0|     0|      0|       0|0.5018338429974938|\n",
      "|   7|76887|              1.0|               0.2|      1|     0|     0|     1|      0|       0|0.5018338429974938|\n",
      "|   8|11109|              1.0|               0.0|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|  10|12081|              1.0| 0.111111111111111|      1|     1|     0|     0|      0|       0|0.5018338429974938|\n",
      "|  10|18840|              1.0|0.0769230769230769|      1|     1|     0|     0|      0|       0|0.5018338429974938|\n",
      "|  10|50307|              0.0| 0.285714285714286|      1|     0|     0|     0|      0|       0|0.5018338429974938|\n",
      "|  11| 2800|              1.0|             0.125|      1|     1|     1|     0|      0|       0|0.5018338429974938|\n",
      "|  11|18771|              0.4|             0.125|      0|     0|     0|     1|      0|       0|0.5018338429974938|\n",
      "|  11|52065|              1.0|              0.25|      0|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|  12|53294|              1.0| 0.222222222222222|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|  14|95329|0.142857142857143|               1.0|      1|     0|     0|     0|      0|       0|0.5018338429974938|\n",
      "|  18|51329|              1.0| 0.166666666666667|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|  18|92932|              1.0|             0.125|      1|     0|     0|     1|      0|       0|0.5018338429974938|\n",
      "|  19|53738|              1.0|             0.375|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|  21|22765|              0.4|               0.0|      1|     1|     0|     0|      0|       0|0.5018338429974938|\n",
      "|  21|41956|              0.4|               0.0|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "|  21|56914|              1.0|               0.0|      1|     0|     1|     0|      0|       0|0.5018338429974938|\n",
      "+----+-----+-----------------+------------------+-------+------+------+------+-------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------------+-----------------+-------+------+------+------+-------+--------+------------------+\n",
      "|id_1| id_2|     cmp_fname_c1|     cmp_lname_c1|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|      class_weight|\n",
      "+----+-----+-----------------+-----------------+-------+------+------+------+-------+--------+------------------+\n",
      "|   5|    6|              1.0|              1.0|      1|     1|     1|     1|      1|       1| 138.6079865016873|\n",
      "|  18|69293|              1.0|              0.0|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  19| 1961|0.222222222222222|0.555555555555556|      1|     1|     1|     1|      0|       0|0.5018101777191354|\n",
      "|  19|78044|0.222222222222222|             0.25|      0|     1|     1|     1|      0|       0|0.5018101777191354|\n",
      "|  21|86148|              0.4|             0.25|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  26|46196|0.333333333333333|              0.5|      1|     0|     0|     0|      0|       0|0.5018101777191354|\n",
      "|  30|92604|              1.0|              1.0|      1|     0|     0|     0|      0|       0|0.5018101777191354|\n",
      "|  31|66720|              1.0|              0.2|      1|     1|     0|     0|      0|       0|0.5018101777191354|\n",
      "|  39|17763|              1.0|              0.0|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  41|19444|              1.0|0.428571428571429|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  46|94234|              1.0|              0.0|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  47| 1050|              1.0|             0.25|      1|     0|     0|     1|      0|       0|0.5018101777191354|\n",
      "|  49|14401|0.142857142857143|              0.4|      1|     0|     0|     0|      0|       0|0.5018101777191354|\n",
      "|  49|78342|              0.0|              0.8|      1|     0|     0|     0|      0|       0|0.5018101777191354|\n",
      "|  49|87587|              1.0|              0.0|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  50|84013|              1.0|             0.25|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  55|86490|0.166666666666667|              1.0|      1|     0|     0|     0|      0|       0|0.5018101777191354|\n",
      "|  57|45401|              1.0|0.142857142857143|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  57|53540|              0.8|0.111111111111111|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "|  57|79388|              1.0|0.166666666666667|      1|     0|     1|     0|      0|       0|0.5018101777191354|\n",
      "+----+-----+-----------------+-----------------+-------+------+------+------+-------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_class_weight = balance_dataset(train_imputed_2, label_col='is_match')\n",
    "train_class_weight.show()\n",
    "# train_class_weight.coalesce(1).write.csv(\"train_final\")\n",
    "\n",
    "test_class_weight = balance_dataset(test_imputed_2, label_col='is_match')\n",
    "test_class_weight.show()\n",
    "# test_class_weight.coalesce(1).write.csv(\"test_final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41614227f7003fb2ad7e84c3c97020ca2cafb1d517e9fb3ef4712a92261ef378"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
